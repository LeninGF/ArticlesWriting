@article{yosinski2014transferable,
	title        = {How transferable are features in deep neural networks?},
	author       = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year         = 2014,
	journal      = {Advances in neural information processing systems},
	volume       = 27
}

@misc{howard2018universallanguagemodelfinetuning,
	title        = {Universal Language Model Fine-tuning for Text Classification},
	author       = {Jeremy Howard and Sebastian Ruder},
	year         = 2018,
	url          = {https://arxiv.org/abs/1801.06146},
	eprint       = {1801.06146},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{Zahangir2018,
	title        = {The History Began from AlexNet: {A} Comprehensive Survey on Deep Learning Approaches},
	author       = {Md. Zahangir Alom and Tarek M. Taha and Christopher Yakopcic and Stefan Westberg and Paheding Sidike and Mst Shamima Nasrin and Brian C. Van Essen and Abdul A. S. Awwal and Vijayan K. Asari},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1803.01164},
	url          = {http://arxiv.org/abs/1803.01164},
	eprinttype   = {arXiv},
	eprint       = {1803.01164},
	timestamp    = {Wed, 24 Mar 2021 17:48:37 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-01164.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{Kitchenham2007,
	title        = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
	author       = {Kitchenham, Barbara and Charters, S},
	year         = 2007,
	booktitle    = {Engineering},
	volume       = 2,
	pages        = 1051,
	doi          = {10.1145/1134285.1134500},
	isbn         = 1595933751,
	issn         = {00010782},
	abstract     = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
	archiveprefix = {arXiv},
	arxivid      = {1304.1186},
	eprint       = {1304.1186},
	mendeley-groups = {Tarea 3},
	pmid         = 10853839
}
@article{PATHAK20181706,
	title        = {Application of Deep Learning for Object Detection},
	author       = {Ajeet Ram Pathak and Manjusha Pandey and Siddharth Rautaray},
	year         = 2018,
	journal      = {Procedia Computer Science},
	volume       = 132,
	pages        = {1706--1717},
	doi          = {https://doi.org/10.1016/j.procs.2018.05.144},
	issn         = {1877-0509},
	url          = {https://www.sciencedirect.com/science/article/pii/S1877050918308767},
	note         = {International Conference on Computational Intelligence and Data Science},
	keywords     = {Object detection, Computer vision, Deep learning, Convolutional neural network},
	abstract     = {The ubiquitous and wide applications like scene understanding, video surveillance, robotics, and self-driving systems triggered vast research in the domain of computer vision in the most recent decade. Being the core of all these applications, visual recognition systems which encompasses image classification, localization and detection have achieved great research momentum. Due to significant development in neural networks especially deep learning, these visual recognition systems have attained remarkable performance. Object detection is one of these domains witnessing great success in computer vision. This paper demystifies the role of deep learning techniques based on convolutional neural network for object detection. Deep learning frameworks and services available for object detection are also enunciated. Deep learning techniques for state-of-the-art object detection systems are assessed in this paper.}
}
@article{radford2019language,
	title        = {Language models are unsupervised multitask learners},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	year         = 2019,
	journal      = {OpenAI blog},
	volume       = 1,
	number       = 8,
	pages        = 9
}
@misc{heaton2018ian,
	title        = {Ian goodfellow, yoshua bengio, and aaron courville: Deep learning},
	author       = {Heaton, Jeff},
	year         = 2018,
	publisher    = {Springer}
}
@book{murphy2022probabilistic,
	title        = {Probabilistic machine learning: an introduction},
	author       = {Murphy, Kevin P},
	year         = 2022,
	publisher    = {MIT press}
}
@article{Sanh2019DistilBERTAD,
	title        = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author       = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year         = 2019,
	journal      = {ArXiv},
	volume       = {abs/1910.01108}
}
@book{tunstall2022natural,
	title        = {Natural language processing with transformers},
	author       = {Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},
	year         = 2022,
	publisher    = {" O'Reilly Media, Inc."}
}
@article{Minaee2021,
	title        = {Deep Learning--based Text Classification},
	author       = {Shervin Minaee and Nal Kalchbrenner and Erik Cambria and Narjes Nikzad and Meysam Chenaghlu and Jianfeng Gao},
	year         = 2021,
	month        = 4,
	journal      = {ACM Computing Surveys (CSUR)},
	publisher    = {ACM PUB27 New York, NY, USA},
	volume       = 54,
	doi          = {10.1145/3439726},
	issn         = 15577341,
	url          = {https://dl.acm.org/doi/10.1145/3439726},
	abstract     = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, ...},
	issue        = 3,
	keywords     = {Text classification,deep learning,natural language inference,news categorization,question answering,sentiment analysis,topic classification}
}
@article{Marcin2018,
	title        = {A recent overview of the state-of-the-art elements of text classification},
	author       = {Marcin Michał Mirończuk and Jarosław Protasiewicz},
	year         = 2018,
	month        = 9,
	journal      = {Expert Systems with Applications},
	publisher    = {Pergamon},
	volume       = 106,
	pages        = {36--54},
	doi          = {10.1016/J.ESWA.2018.03.058},
	issn         = {0957-4174},
	abstract     = {The aim of this study is to provide an overview the state-of-the-art elements of text classification. For this purpose, we first select and investigate the primary and recent studies and objectives in this field. Next, we examine the state-of-the-art elements of text classification. In the following steps, we qualitatively and quantitatively analyse the related works. Herein, we describe six baseline elements of text classification including data collection, data analysis for labelling, feature construction and weighing, feature selection and projection, training of a classification model, and solution evaluation. This study will help readers acquire the necessary information about these elements and their associated techniques. Thus, we believe that this study will assist other researchers and professionals to propose new studies in the field of text classification.},
	keywords     = {Document classification,Document classification overview,Text classification,Text classification overview}
}
@article{Kowsari2019,
	title        = {Text Classification Algorithms: A Survey},
	author       = {Kamran Kowsari and Kiana Jafari Meimandi and Mojtaba Heidarysafa and Sanjana Mendu and Laura Barnes and Donald Brown},
	year         = 2019,
	doi          = {10.3390/info10040150},
	url          = {www.mdpi.com/journal/information},
	abstract     = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
	keywords     = {document classification,text analysis,text categorization,text classification,text mining,text representation}
}

% the elmo paper
@report{Peters2018,
	title        = {Deep contextualized word representations},
	author       = {Matthew E Peters and Mark Neumann and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
	year         = 2018,
	pages        = {2227--2237},
	url          = {http://allennlp.org/elmo},
	abstract     = {We introduce a new type of deep contextual-ized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirec-tional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, tex-tual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}
the ulmfit paper
@report{howard2018,
	title        = {Universal Language Model Fine-tuning for Text Classification},
	author       = {Jeremy Howard and Sebastian Ruder},
	year         = 2018,
	pages        = {328--339},
	url          = {http://nlp.fast.ai/ulmfit.},
	abstract     = {Inductive transfer learning has greatly im-pacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outper-forms the state-of-the-art on six text classification tasks, reducing the error by 18-\% on the majority of datasets. Furthermore , with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We open-source our pretrained models and code 1 .}
}
@misc{lexglue2021,
	title        = {LexGLUE: A Benchmark Dataset for Legal Language Understanding in English},
	author       = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel Martin and Aletras, Nikolaos},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2110.00976},
	url          = {https://arxiv.org/abs/2110.00976},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
% ===== tesis =====
@mastersthesis{falconi2020transfer,
	title        = {Transfer and ensemble learning models in breast mammogram pathology classification},
	author       = {Lenin G. Falcon{í}},
	year         = 2020,
	school       = {Quito, 2020.}
}
% ========= machine learning y leyes ================
@article{mumcuouglu2021natural,
	title        = {Natural language processing in law: Prediction of outcomes in the higher courts of Turkey},
	author       = {Mumcuo{\u{g}}lu, Emre and {\"O}zt{\"u}rk, Ceyhun E and Ozaktas, Haldun M and Ko{\c{c}}, Aykut},
	year         = 2021,
	journal      = {Information Processing \& Management},
	publisher    = {Elsevier},
	volume       = 58,
	number       = 5,
	pages        = 102684
}
@inproceedings{yan2019law,
	title        = {Law article prediction based on deep learning},
	author       = {Yan, Ge and Li, Yu and Shen, Siyuan and Zhang, Shu and Liu, Jia},
	year         = 2019,
	booktitle    = {2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
	pages        = {281--284},
	organization = {IEEE}
}
@incollection{kalia2022classifying,
	title        = {Classifying Case Facts and Predicting Legal Decisions of the Indian Central Information Commission: a Natural Language Processing Approach},
	author       = {Kalia, Arvind and Kumar, Naveen and Namdev, Nischay},
	year         = 2022,
	booktitle    = {Advances in Deep Learning, Artificial Intelligence and Robotics},
	publisher    = {Springer},
	pages        = {35--45}
}
@inproceedings{wang2020deep,
	title        = {Deep learning algorithm for judicial judgment prediction based on BERT},
	author       = {Wang, Yongjun and Gao, Jing and Chen, Junjie},
	year         = 2020,
	booktitle    = {2020 5th International Conference on Computing, Communication and Security (ICCCS)},
	pages        = {1--6},
	organization = {IEEE}
}
@report{clavie2021,
	title        = {The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification},
	author       = {Benjamin Clavié and Marc Alphonsus and Clavi´ Clavié and Jus Mundi},
	year         = 2021,
	url          = {https://gitlab.com/jusmundi-group/public/Legal-svm-baselines},
	abstract     = {We aim to highlight an interesting trend to contribute to the ongoing debate around advances within legal Natural Language Processing. Recently, the focus for most legal text classification tasks has shifted towards large pre-trained deep learning models such as BERT. In this paper, we show that a more traditional approach based on Support Vector Machine classifiers reaches surprisingly competitive performance with BERT-based models on the classification tasks in the LexGLUE benchmark. We also highlight that error reduction obtained by using specialised BERT-based models over baselines is noticeably smaller in the legal domain when compared to general language tasks. We present and discuss three hypotheses as potential explanations for these results to support future discussions.},
	keywords     = {Machine Learning,Natural Language Processing,Text Classification}
}

% ==================== Recommneded by DeepSeek ====================
@article{dinardap2020,
	title        = {Norma de Digitalización de Documentos de la DINARDAP},
	author       = {DINARDAP},
	year         = 2016,
	note         = {Resolución de la DINARDAP 20},
	url          = {https://rpmr.gob.ec/wp-content/uploads/2021/09/NORMA-DE-DIGITALIZACION-DE-DOCUMENTOS-DE-LA-DINARDAP.pdf}
}


@article{pires2019multilingual,
	title        = {How multilingual is multilingual BERT?},
	author       = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
	year         = 2019,
	journal      = {ACL}
}
@article{chalkidis2020legal,
	title        = {Legal-BERT: The Muppets straight out of Law School},
	author       = {Chalkidis, Ilias and Fergadiotis, Manos and Androutsopoulos, Ion},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.02559}
}
@article{liu2022legal,
	title        = {Legal Knowledge-enhanced Language Models for Legal Text Processing},
	author       = {Liu, Zihan and Li, Zheng and Zhang, Hongye},
	year         = 2022,
	journal      = {ICAIL}
}
@article{wu2016google,
	title        = {Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
	author       = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and others},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1609.08144}
}


@misc{coip2023,
	title        = {C{\'o}digo Org{\'a}nico Integral Penal (COIP)},
	author       = {{Asamblea Nacional del Ecuador}},
	year         = 2021,
	url          = {https://www.defensa.gob.ec/wp-content/uploads/downloads/2021/03/COIP_act_feb-2021.pdf},
	note         = {Ley 0, Registro Oficial Suplemento 180, Última modificación: 17-feb-2021},
	howpublished = {Documento en línea}
}

@article{savelka2021cross,
	title        = {Cross-Domain Generalization and Knowledge Transfer in Transformers for Legal Text},
	author       = {Savelka, Jaromir and Ashley, Kevin D},
	year         = 2021,
	journal      = {ICAIL}
}
@inproceedings{devlin2018bert,
	title        = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {4171--4186},
	doi          = {10.18653/v1/N19-1423},
	url          = {https://aclanthology.org/N19-1423/},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{vaswani2017attention,
	title        = {Attention Is All You Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
	year         = 2017,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 30
}

% ==================== gemini canvas literature found ====================
@article{Allam2025,
	title        = {Text Classification: How Machine Learning Is Revolutionizing Text Categorization},
	author       = {Hesham Allam and Lisa Makubvure and Benjamin Gyamfi and Kwadwo Nyarko Graham and Kehinde Akinwolere},
	year         = 2025,
	month        = 2,
	journal      = {Information 2025, Vol. 16, Page 130},
	publisher    = {Multidisciplinary Digital Publishing Institute},
	volume       = 16,
	pages        = 130,
	doi          = {10.3390/INFO16020130},
	issn         = {2078-2489},
	url          = {https://www.mdpi.com/2078-2489/16/2/130/htm https://www.mdpi.com/2078-2489/16/2/130},
	abstract     = {The automated classification of texts into predefined categories has become increasingly prominent, driven by the exponential growth of digital documents and the demand for efficient organization. This paper serves as an in-depth survey of text classification and machine learning, consolidating diverse aspects of the field into a single, comprehensive resource—a rarity in the current body of literature. Few studies have achieved such breadth, and this work aims to provide a unified perspective, offering a significant contribution to researchers and the academic community. The survey examines the evolution of machine learning in text categorization (TC), highlighting its transformative advantages over manual classification, such as enhanced accuracy, reduced labor, and adaptability across domains. It delves into various TC tasks and contrasts machine learning methodologies with knowledge engineering approaches, demonstrating the strengths and flexibility of data-driven techniques. Key applications of TC are explored, alongside an analysis of critical machine learning methods, including document representation techniques and dimensionality reduction strategies. Moreover, this study evaluates a range of text categorization models, identifies persistent challenges like class imbalance and overfitting, and investigates emerging trends shaping the future of the field. It discusses essential components such as document representation, classifier construction, and performance evaluation, offering a well-rounded understanding of the current state of TC. Importantly, this paper also provides clear research directions, emphasizing areas requiring further innovation, such as hybrid methodologies, explainable AI (XAI), and scalable approaches for low-resource languages. By bridging gaps in existing knowledge and suggesting actionable paths forward, this work positions itself as a vital resource for academics and industry practitioners, fostering deeper exploration and development in text classification.},
	issue        = 2,
	keywords     = {automation,classifier evaluation,dimension reduction,document representation,emerging trends,machine learning,text categorization (TC)}
}

@article{Ariai2024,
	title        = {Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges},
	author       = {Farid Ariai and Gianluca Demartini},
	year         = 2024,
	month        = 10,
	journal      = {ACM Computing Surveys},
	volume       = 1,
	doi          = {10.1145/nnnnnnn.nnnnnnn},
	url          = {https://arxiv.org/abs/2410.21306v2},
	abstract     = {Natural Language Processing (NLP) is revolutionising the way legal professionals and laypersons operate in the legal field. The considerable potential for NLP in the legal sector, especially in developing computational tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 133 after manual filtering. It explores foundational concepts related to NLP in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document length, complex language, and limited open legal datasets. We provide an overview of NLP tasks specific to legal text, such as Legal Document Summarisation, legal Named Entity Recognition, Legal Question Answering, Legal Argument Mining, Legal Text Classification, and Legal Judgement Prediction. In the section on legal Language Models (LMs), we analyse both developed LMs and approaches for adapting general LMs to the legal domain. Additionally, we identify 16 Open Research Challenges, including bias in Artificial Intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.},
	keywords     = {Artificial Intelligence,Law,Legal Domain,Natural Language Processing}
}

% ===== this one from scispace =====
@inproceedings{Gosh_Kumar_2024,
	title        = {Evaluating Transformer Models for Legal Judgement Prediction: A Comparative Study},
	author       = {Ghosh, Trisha and Kumar, Shailender},
	year         = 2024,
	month        = 6,
	booktitle    = {2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
	volume       = {},
	number       = {},
	pages        = {1--4},
	doi          = {10.1109/ICCCNT61001.2024.10725043},
	issn         = {2473-7674},
	abstract     = {Current developments in natural language processing (NLP) show that there is great potential in the area of law. This study investigates the performance of six transformer-based models BERT, XLNet, RoBERTa, DeBERTa, ELECTRA, and BigBird in predicting legal decisions using ILDC (Indian Legal Documents Corpus)-single dataset. Given the complexity of Indian judiciary, our study examines how recent advances in transformer technology can enhance classification accuracy. Our preliminary findings indicate that the best performance from the newer models (DeBERTa, ELECTRA, BigBird) reaches 80\% accuracy, while the old models peaked at 76\% accuracy, which is almost a 4\% increment. This paper provides a detailed comparative analysis of each model, focusing on their respective merits and potential for automating legal judgements. In addition, we address the shortcomings of our work and make recommendations for further research.},
	keywords     = {Analytical models;Electric potential;Accuracy;Law;Computational modeling;Predictive models;Transformers;Data models;Natural language processing;Reliability;Natural Language Processing;Transformer models;Court Judgement Prediction;Legal AI applications;BERT;XLNet;ROBERTa;DeBERTa;ELECTRA;BigBird;Indian Law}
}

% ===== from Deep Seek =====
@article{Zhong2020,
   abstract = {Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we describe the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.},
   author = {Haoxi Zhong and Chaojun Xiao and Cunchao Tu and Tianyang Zhang and Zhiyuan Liu and Maosong Sun},
   doi = {10.18653/V1/2020.ACL-MAIN.466},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {5218-5230},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence},
   url = {https://aclanthology.org/2020.acl-main.466/},
   year = {2020}
}
@article{Katz2023,
   abstract = {In this paper, we summarize the current state of the field of NLP & Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP & Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
   author = {Daniel Martin Katz and Dirk Hartung and Lauritz Gerlach and Abhik Jana and Michael J. Bommarito II},
   doi = {10.2139/ssrn.4336224},
   journal = {SSRN Electronic Journal},
   month = {2},
   publisher = {Elsevier BV},
   title = {Natural Language Processing in the Legal Domain},
   url = {https://arxiv.org/abs/2302.12039v1},
   year = {2023}
}

@article{Abnar2020,
	title        = {Quantifying Attention Flow in Transformers},
	author       = {Samira Abnar and Willem Zuidema},
	year         = 2020,
	month        = 5,
	journal      = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics (ACL)},
	pages        = {4190--4197},
	doi          = {10.18653/v1/2020.acl-main.385},
	isbn         = 9781952148255,
	issn         = {0736587X},
	url          = {https://arxiv.org/abs/2005.00928v2},
	abstract     = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.}
}

@misc{ribeiro2016,
	title        = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	author       = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
	year         = 2016,
	url          = {https://arxiv.org/abs/1602.04938},
	eprint       = {1602.04938},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@article{Martinez2024,
	title        = {Re-evaluating GPT-4’s bar exam performance},
	author       = {Eric Martínez},
	year         = 2024,
	month        = 3,
	journal      = {Artificial Intelligence and Law},
	publisher    = {Springer Nature},
	pages        = {1--24},
	doi          = {10.1007/S10506-024-09396-9/FIGURES/1},
	issn         = 15728382,
	url          = {https://link.springer.com/article/10.1007/s10506-024-09396-9},
	abstract     = {Perhaps the most widely touted of GPT-4’s at-launch, zero-shot capabilities has been its reported 90th-percentile performance on the Uniform Bar Exam. This paper begins by investigating the methodological challenges in documenting and verifying the 90th-percentile claim, presenting four sets of findings that indicate that OpenAI’s estimates of GPT-4’s UBE percentile are overinflated. First, although GPT-4’s UBE score nears the 90th percentile when examining approximate conversions from February administrations of the Illinois Bar Exam, these estimates are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population. Second, data from a recent July administration of the same exam suggests GPT-4’s overall UBE percentile was below the 69th percentile, and ∼48th percentile on essays. Third, examining official NCBE data and using several conservative statistical assumptions, GPT-4’s performance against first-time test takers is estimated to be ∼62nd percentile, including ∼42nd percentile on essays. Fourth, when examining only those who passed the exam (i.e. licensed or license-pending attorneys), GPT-4’s performance is estimated to drop to ∼48th percentile overall, and ∼15th percentile on essays. In addition to investigating the validity of the percentile claim, the paper also investigates the validity of GPT-4’s reported scaled UBE score of 298. The paper successfully replicates the MBE score, but highlights several methodological issues in the grading of the MPT + MEE components of the exam, which call into question the validity of the reported essay score. Finally, the paper investigates the effect of different hyperparameter combinations on GPT-4’s MBE performance, finding no significant effect of adjusting temperature settings, and a significant effect of few-shot chain-of-thought prompting over basic zero-shot prompting. Taken together, these findings carry timely insights for the desirability and feasibility of outsourcing legally relevant tasks to AI models, as well as for the importance for AI developers to implement rigorous and transparent capabilities evaluations to help secure safe and trustworthy AI.},
	keywords     = {Artificial intelligence,Artificial intelligence and law,Law and technology,Legal NLP,Legal analytics,Legal profession,Machine learning,NLP,Natural language processing}
}

@article{Ouyang2022,
	title        = {Training language models to follow instructions with human feedback},
	author       = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
	year         = 2022,
	month        = 3,
	journal      = {Advances in Neural Information Processing Systems},
	publisher    = {Neural information processing systems foundation},
	volume       = 35,
	isbn         = 9781713871088,
	issn         = 10495258,
	url          = {https://arxiv.org/abs/2203.02155v1},
	abstract     = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.}
}

@inproceedings{radford2018gpt,
	title        = {Improving Language Understanding by Generative Pre-Training},
	author       = {Alec Radford and Karthik Narasimhan},
	year         = 2018,
	url          = {https://api.semanticscholar.org/CorpusID:49313245}
}
@article{openAI2023,
	title        = {GPT-4 Technical Report},
	author       = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
	year         = 2023,
	month        = 3,
	url          = {https://arxiv.org/abs/2303.08774v6},
	abstract     = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.}
}
@article{Luo2022NLP,
	title        = {Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering},
	author       = {Man Luo and Kazuma Hashimoto and Semih Yavuz and Zhiwei Liu and Chitta Baral and Yingbo Zhou},
	year         = 2022,
	month        = 3,
	journal      = {Spa-NLP 2022 - 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge, Proceedings of the Workshop},
	publisher    = {Association for Computational Linguistics (ACL)},
	pages        = {7--22},
	doi          = {10.18653/v1/2022.spanlp-1.2},
	isbn         = 9781955917506,
	url          = {https://arxiv.org/abs/2203.07522v1},
	abstract     = {While both extractive and generative readers have been successfully applied to the Question Answering (QA) task, little attention has been paid toward the systematic comparison of them. Characterizing the strengths and weaknesses of the two readers is crucial not only for making a more informed reader selection in practice but also for developing a deeper understanding to foster further research on improving readers in a principled manner. Motivated by this goal, we make the first attempt to systematically study the comparison of extractive and generative readers for question answering. To be aligned with the state-of-the-art, we explore nine transformer-based large pre-trained language models (PrLMs) as backbone architectures. Furthermore, we organize our findings under two main categories: (1) keeping the architecture invariant, and (2) varying the underlying PrLMs. Among several interesting findings, it is important to highlight that (1) the generative readers perform better in long context QA, (2) the extractive readers perform better in short context while also showing better out-of-domain generalization, and (3) the encoder of encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also study the effect of multi-task learning on the two types of readers varying the underlying PrLMs and perform qualitative and quantitative diagnosis to provide further insights into future directions in modeling better readers.}
}

% gemini flash 2.0 april 20, 2025
@article{Akca2022,
	title        = {Traditional Machine Learning and Deep Learning-based Text Classification for Turkish Law Documents using Transformers and Domain Adaptation},
	author       = {Onur Akca and Giyaseddin Bayrak and Abdul Majeed Issifu and Murat Can Ganiz},
	year         = 2022,
	journal      = {16th International Conference on INnovations in Intelligent SysTems and Applications, INISTA 2022},
	publisher    = {Institute of Electrical and Electronics Engineers Inc.},
	doi          = {10.1109/INISTA55318.2022.9894051},
	isbn         = 9781665498104,
	abstract     = {Natural Language Processing (NLP) is an interdisciplinary field between linguistics and computer science. Its main aim is to process natural (human) language using computer programs. Text classification is one of the main tasks of this field, and they are widely used in many different applications such as spam filtering, sentiment analysis, and document categorization. Nonetheless, there is only very little text classification work in the law domain and even less for the Turkish language. This may be attributed to the complexity within the domain. The length, complexity of documents, and use of extensive technical jargon are some of the reasons that distinguish this domain from others. Similar to the medical domain, understanding these documents requires extensive specialization. Another reason can be the scarcity of publicly available datasets. In this study, we compile sizeable unsupervised and supervised datasets from publicly available sources and experiment with several classification algorithms ranging from traditional classifiers to much more complicated deep learning and transformer-based models along with different text representations. We focus on classifying Court of Cassation decisions for their crime labels. Interestingly, the majority of the models we experiment with could be able to obtain good results. This suggests that although understanding the documents in the legal domain is complicated and requires expertise from humans, it may be relatively easier for machine learning models despite the extensive presence of the technical terms. This seems to be especially the case for transformer-based pre-trained neural language models which can be adapted to the law domain, showing high potential for future real-world applications.},
	keywords     = {Domain-specific language models,Legal document classification,Natural Language Processing}
}
@article{Noguti2024,
	title        = {A Small Claims Court for the NLP: Judging Legal Text Classification Strategies With Small Datasets},
	author       = {Mariana Yukari Noguti and Edduardo Vellasques and Luiz Eduardo Soares Oliveira},
	year         = 2024,
	month        = 9,
	doi          = {10.1109/SMC53992.2023.10394189},
	url          = {http://arxiv.org/abs/2409.05972 http://dx.doi.org/10.1109/SMC53992.2023.10394189},
	abstract     = {Recent advances in language modelling has significantly decreased the need of labelled data in text classification tasks. Transformer-based models, pre-trained on unlabeled data, can outmatch the performance of models trained from scratch for each task. However, the amount of labelled data need to fine-tune such type of model is still considerably high for domains requiring expert-level annotators, like the legal domain. This paper investigates the best strategies for optimizing the use of a small labeled dataset and large amounts of unlabeled data and perform a classification task in the legal area with 50 predefined topics. More specifically, we use the records of demands to a Brazilian Public Prosecutor's Office aiming to assign the descriptions in one of the subjects, which currently demands deep legal knowledge for manual filling. The task of optimizing the performance of classifiers in this scenario is especially challenging, given the low amount of resources available regarding the Portuguese language, especially in the legal domain. Our results demonstrate that classic supervised models such as logistic regression and SVM and the ensembles random forest and gradient boosting achieve better performance along with embeddings extracted with word2vec when compared to BERT language model. The latter demonstrates superior performance in association with the architecture of the model itself as a classifier, having surpassed all previous models in that regard. The best result was obtained with Unsupervised Data Augmentation (UDA), which jointly uses BERT, data augmentation, and strategies of semi-supervised learning, with an accuracy of 80.7\% in the aforementioned task.}
}
@article{Shaheen2021,
	title        = {Zero-Shot Cross-Lingual Transfer in Legal Domain Using Transformer Models},
	author       = {Zein Shaheen and Gerhard Wohlgenannt and Dmitry Mouromtsev},
	year         = 2021,
	month        = 11,
	journal      = {Proceedings - 2021 International Conference on Computational Science and Computational Intelligence, CSCI 2021},
	publisher    = {Institute of Electrical and Electronics Engineers Inc.},
	pages        = {450--456},
	doi          = {10.1109/CSCI54926.2021.00145},
	isbn         = 9781665458412,
	url          = {https://arxiv.org/abs/2111.14192v2},
	abstract     = {Zero-shot cross-lingual transfer is an important feature in modern NLP models and architectures to support low-resource languages. In this work, We study zero-shot cross-lingual transfer from English to French and German under Multi-Label Text Classification, where we train a classifier using English training set, and we test using French and German test sets. We extend EURLEX57K dataset, the English dataset for topic classification of legal documents, with French and German official translation. We investigate the effect of using some training techniques, namely Gradual Unfreezing and Language Model finetuning, on the quality of zero-shot cross-lingual transfer. We find that Language model finetuning of multi-lingual pre-trained model (M-DistilBERT, M-BERT) leads to 32.0-34.94\%, 76.15-87.54\% relative improvement on French and German test sets correspondingly. Also, Gradual unfreezing of pre-trained model's layers during training results in relative improvement of 38-45\% for French and 58-70\% for German. Compared to training a model in Joint Training scheme using English, French and German training sets, zero-shot BERT-based classification model reaches 86\% of the performance achieved by jointly-trained BERT-based classification model.},
	keywords     = {Natural Language Processing,Transfer Learning,Transformer Models,Zero-shot classification}
}
@article{Shaheen2020,
	title        = {Large Scale Legal Text Classification Using Transformer Models},
	author       = {Zein Shaheen and Gerhard Wohlgenannt and Erwin Filtz},
	year         = 2020,
	month        = 10,
	url          = {https://arxiv.org/abs/2010.12871v1},
	abstract     = {Large multi-label text classification is a challenging Natural Language Processing (NLP) problem that is concerned with text classification for datasets with thousands of labels. We tackle this problem in the legal domain, where datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc vocabulary were created within the legal information systems of the European Union. The EuroVoc taxonomy includes around 7000 concepts. In this work, we study the performance of various recent transformer-based models in combination with strategies such as generative pretraining, gradual unfreezing and discriminative learning rates in order to reach competitive classification performance, and present new state-of-the-art results of 0.661 (F1) for JRC-Acquis and 0.754 for EURLEX57K. Furthermore, we quantify the impact of individual steps, such as language model fine-tuning or gradual unfreezing in an ablation study, and provide reference dataset splits created with an iterative stratification algorithm.},
	keywords     = {EuroVoc,legal document datasets,multi-label text classification,transformer models}
}
@article{Vatsal2023,
	title        = {Classification of US Supreme Court Cases using BERT-Based Techniques},
	author       = {Shubham Vatsal and Adam Meyers and John E Ortega},
	year         = 2023,
	url          = {https://huggingface.co/saibo/},
	abstract     = {Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories which marks an improvement of 8\% and 28\% respectively from previously reported SOTA results.}
}

% ==================== from the finetuning section of report ====================
@misc{accuracyParadoxWikipedia2022,
	title        = {Accuracy paradox --- {Wikipedia}{,} The Free Encyclopedia},
	author       = {{Wikipedia contributors}},
	year         = 2022,
	url          = {https://en.wikipedia.org/w/index.php?title=Accuracy_paradox&oldid=1120442750},
	note         = {[Online; accessed 28-December-2022]}
}
@misc{wikipedia_tpu,
	title        = {Unidad de procesamiento tensorial --- Wikipedia{,} La enciclopedia libre},
	author       = {Wikipedia},
	year         = 2024,
	url          = {\url{https://es.wikipedia.org/w/index.php?title=Unidad_de_procesamiento_tensorial&oldid=157891658}},
	note         = {[Internet; descargado 31-enero-2024]}
}
@misc{torx2023,
	title        = {¿Qué es una Unidad de procesamiento tensorial y cómo funciona?},
	author       = {Max Torx},
	year         = 2023,
	month        = may,
	url          = {"https://tensorprocessingunit.com/que-es-una-unidad-de-procesamiento-tensorial-y-como-funciona/"}
}

% for accuracy paradox
@article{abma2009evaluation,
	title        = {Evaluation of requirements management tools with support for traceability-based change impact analysis},
	author       = {Abma, BJM},
	year         = 2009,
	journal      = {Master's thesis, University of Twente, Enschede}
}
% for tpus

@misc{wang2019benchmarkingtpugpucpu,
	title        = {Benchmarking TPU, GPU, and CPU Platforms for Deep Learning},
	author       = {Yu Emma Wang and Gu-Yeon Wei and David Brooks},
	year         = 2019,
	url          = {https://arxiv.org/abs/1907.10701},
	eprint       = {1907.10701},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{carrion2023explorationtpusaiapplications,
	title        = {Exploration of TPUs for AI Applications},
	author       = {Diego Sanmartín Carrión and Vera Prohaska},
	year         = 2023,
	url          = {https://arxiv.org/abs/2309.08918},
	eprint       = {2309.08918},
	archiveprefix = {arXiv},
	primaryclass = {cs.AR}
}

@misc{raffel2023T5,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2023,
	url          = {https://arxiv.org/abs/1910.10683},
	eprint       = {1910.10683},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

% ==================== from QA paper
@inproceedings{jha2022extractive,
  title={Extractive Question Answering Using Transformer-Based LM},
  author={Jha, Raj and Devi, V Susheela},
  booktitle={International Conference on Neural Information Processing},
  pages={373--384},
  year={2022},
  organization={Springer}
}

@article{Pearce_Zhan_Komanduri_Zhan_2021,
	title        = {A Comparative Study of Transformer-Based Language Models on Extractive Question Answering},
	author       = {Pearce, Kate and Zhan, Tiffany and Komanduri, Aneesh and Zhan, Justin},
	year         = 2021,
	month        = oct,
	journal      = {arXiv: Computation and Language},
	url          = {http://export.arxiv.org/pdf/2110.03142}
}

@article{Vold_Conrad_2021,
	title        = {Using transformers to improve answer retrieval for legal questions},
	author       = {Vold, Andrew and Conrad, Jack G.},
	year         = 2021,
	month        = jun,
	journal      = {International Conference on Artificial Intelligence and Law},
	publisher    = {ACM},
	pages        = {245–249},
	doi          = {10.1145/3462757.3466102},
	url          = {https://dl.acm.org/doi/pdf/10.1145/3462757.3466102}
}

@article{Shao_Guo_Chen_Zepeng_2019,
	title        = {Transformer-Based Neural Network for Answer Selection in Question Answering},
	author       = {Shao, Taihua and Guo, Yupu and Chen, Honghui and Zepeng, Hao},
	year         = 2019,
	month        = feb,
	journal      = {IEEE Access},
	publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
	volume       = 7,
	pages        = {26146–26156},
	doi          = {10.1109/ACCESS.2019.2900753},
	url          = {https://dblp.uni-trier.de/db/journals/access/access7.html#ShaoGCH19}
}
@article{Kim_Rabelo_Babiker_Rahman_Goebel_2024,
	title        = {Legal Information Retrieval and Entailment Using Transformer-based Approaches},
	author       = {Kim, Mi Young and Rabelo, Juliano and Babiker, Housam Khalifa Bashier and Rahman, Md Abed and Goebel, Randy},
	year         = 2024,
	month        = jan,
	journal      = {The Review of Socionetwork Strategies},
	doi          = {10.1007/s12626-023-00153-z},
	url          = {https://link.springer.com/content/pdf/10.1007/s12626-023-00153-z.pdf}
}
@article{Peric_Mijic_Stammbach_Ash_2020,
	title        = {Legal Language Modeling with Transformers},
	author       = {Peric, Lazar and Mijic, Stefan and Stammbach, Dominik and Ash, Elliott},
	year         = 2020,
	month        = dec,
	journal      = {International Conference on Legal Knowledge and Information Systems},
	publisher    = {CEUR-WS},
	volume       = 2764,
	doi          = {10.3929/ETHZ-B-000456079},
	url          = {http://ceur-ws.org/Vol-2764/paper2.pdf}
}